{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "DATASET_PATH = r\"archive\\covid.csv\"\n",
    "from itertools import chain, combinations\n",
    "RESULTS_FILE = \"results\\\\results_{}_{}.csv\"\n",
    "\n",
    "final_outcomes = ['patient_type',\n",
    "                 'intubed',\n",
    "                 'pneumonia',\n",
    "                 'icu',\n",
    "                'date_died'\n",
    "                 ]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim=5):\n",
    "        super(Classifier, self).__init__()\n",
    "        # parameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = 7\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        ones = torch.ones(self.output_dim)\n",
    "        return x + ones"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Classifier_no(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Classifier_no, self).__init__()\n",
    "        # parameters\n",
    "        self.fc1 = nn.Linear(9, 5)\n",
    "        # self.fc2 = nn.Linear(7, 5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        # x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        ones = torch.ones(5)\n",
    "        return x + ones"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, features, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels # torch.Tensor()# .to(dtype=torch.long) #.type(torch.LongTensor)\n",
    "        self.features = features # torch.Tensor()# .to(dtype=torch.long) #.type(torch.LongTensor)\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.features)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        feature = (self.features[index])\n",
    "\n",
    "        # Load data and get label\n",
    "        y = (self.labels[index])\n",
    "\n",
    "        return torch.Tensor(feature), torch.Tensor(y)\n",
    "  \n",
    "  @staticmethod\n",
    "  def collate_fn(batch):\n",
    "      batch_features = []\n",
    "      batch_y = []\n",
    "      for feature, y in batch:\n",
    "          batch_features.append(feature)\n",
    "          batch_y.append(y)\n",
    "        \n",
    "      return (torch.stack(batch_features), \n",
    "              torch.stack(batch_y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def write_in_all_csvs(csv_writers: list, row):\n",
    "    for writer in csv_writers:\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "def calc_prediction_error(pred_vector, gt_vector):\n",
    "    assert len(pred_vector) == len(gt_vector)\n",
    "    error_vec = np.zeros(len(gt_vector))\n",
    "    for indx in range(len(pred_vector)):\n",
    "        error_vec[indx] = abs(pred_vector[indx] - gt_vector[indx])**2\n",
    "    return error_vec\n",
    "\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "feature: ['diabetes', 'copd', 'asthma', 'inmsupr', 'hypertension', 'cardiovascular', 'obesity', 'renal_chronic', 'tobacco']",
      "\n",
      "outcomes: ['patient_type', 'intubed', 'pneumonia', 'icu', 'date_died']",
      "\n",
      "making dataset",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_PATH)\n",
    "df = df.loc[df['covid_res'] == 1]  # take only positives\n",
    "df = df.drop('id', axis=1)  # drop id\n",
    "\n",
    "df.loc[df.date_died == \"9999-99-99\", 'date_died'] = 2\n",
    "df.loc[df.date_died != 2, 'date_died'] = 1\n",
    "df.loc[df.intubed == 97, 'intubed'] = 2\n",
    "df.loc[df.icu == 97, 'icu'] = 2\n",
    "df = df.loc[(df[\"age\"] <= 40) & (df['age'] >= 18)]\n",
    "df = df.loc[(df[\"patient_type\"].isin([1,2])) &\n",
    "            (df['intubed'].isin([1,2])) &\n",
    "            (df['icu'].isin([1,2])) &\n",
    "            (df['date_died'].isin([1,2])) &\n",
    "            (df['pneumonia'].isin([1,2])) &\n",
    "            # (df['sex'].isin([1,2]))\n",
    "            (df['sex'] == 1)\n",
    "            ]\n",
    "df = df.drop('sex', axis=1)  # drop sex\n",
    "features = ['pregnancy', 'diabetes', 'copd', 'asthma',\n",
    "            'inmsupr', 'hypertension',\n",
    "            'cardiovascular', 'obesity', 'renal_chronic',\n",
    "            'tobacco' #,\n",
    "            # 'sex'\n",
    "            ]\n",
    "\n",
    "selected_feature = \"pregnancy\"\n",
    "features.remove(selected_feature)\n",
    "print(f\"feature: {features}\")\n",
    "print(f\"outcomes: {final_outcomes}\")\n",
    "df_yes_feature = df.loc[df[selected_feature] == 1]\n",
    "df_no_feature =  df.loc[df[selected_feature] == 2]\n",
    "\n",
    "print(\"making dataset\")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "features_yes = df_yes_feature[features].to_numpy() # torch.tensor(train_yes[features].to_numpy(), dtype=torch.long) #  train_yes[features].to_numpy() # (dtype=torch.long)\n",
    "labels_yes = df_yes_feature[final_outcomes].to_numpy() # torch.tensor(train_yes[final_outcomes].to_numpy(), dtype=torch.long) # .to(dtype=torch.long)\n",
    "yes_dataset = Dataset(features_yes, labels_yes)\n",
    "yes_dataloader = DataLoader(dataset=yes_dataset, batch_size=batch_size, shuffle=True,\n",
    "                            collate_fn=Dataset.collate_fn)\n",
    "features_no = df_no_feature[features].to_numpy() # torch.tensor(train_yes[features].to_numpy(), dtype=torch.long) #  train_yes[features].to_numpy() # (dtype=torch.long)\n",
    "labels_no = df_no_feature[final_outcomes].to_numpy() # torch.tensor(train_yes[final_outcomes].to_numpy(), dtype=torch.long) # .to(dtype=torch.long)\n",
    "no_dataset = Dataset(features_no, labels_no)\n",
    "no_dataloader = DataLoader(dataset=no_dataset, batch_size=batch_size, shuffle=True,\n",
    "                            collate_fn=Dataset.collate_fn)\n",
    "\n",
    "input_dim = len(features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[1,    22] loss: 0.003",
      "\n",
      "[2,    22] loss: 0.003",
      "\n",
      "[3,    22] loss: 0.003",
      "\n",
      "[4,    22] loss: 0.003",
      "\n",
      "[5,    22] loss: 0.003",
      "\n",
      "[6,    22] loss: 0.003",
      "\n",
      "[7,    22] loss: 0.003",
      "\n",
      "[8,    22] loss: 0.003",
      "\n",
      "[9,    22] loss: 0.003",
      "\n",
      "[10,    22] loss: 0.003",
      "\n",
      "[11,    22] loss: 0.003",
      "\n",
      "[12,    22] loss: 0.003",
      "\n",
      "[13,    22] loss: 0.003",
      "\n",
      "[14,    22] loss: 0.003",
      "\n",
      "[15,    22] loss: 0.003",
      "\n",
      "[16,    22] loss: 0.003",
      "\n",
      "[17,    22] loss: 0.003",
      "\n",
      "[18,    22] loss: 0.003",
      "\n",
      "[19,    22] loss: 0.003",
      "\n",
      "[20,    22] loss: 0.003",
      "\n",
      "[20,    22] loss: 0.003",
      "\n",
      "Finished Training",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# classifier for the yes: \n",
    "model_yes = Classifier(input_dim)\n",
    "\n",
    "criterion_yes = nn.MSELoss()\n",
    "optimizer_yes = optim.SGD(model_yes.parameters(), lr=0.0001, momentum=0.8)\n",
    "\n",
    "total_examples = 0 \n",
    "num_correct = 0 \n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(yes_dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer_yes.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_yes(inputs)\n",
    "        loss = criterion_yes(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_yes.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item() # print every 2000 mini-batches\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            # accuracy = 100 * num_correct / total_examples \n",
    "            # print(f\"Current Accuracy is: {accuracy:.3f}\")\n",
    "\n",
    "# accuracy = 100 * num_correct / total_examples \n",
    "# print(f\"Total Accuracy is: {accuracy:.3f}\")\n",
    "print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[1,   607] loss: 0.013",
      "\n",
      "[2,   607] loss: 0.011",
      "\n",
      "[3,   607] loss: 0.011",
      "\n",
      "[4,   607] loss: 0.011",
      "\n",
      "[5,   607] loss: 0.011",
      "\n",
      "[6,   607] loss: 0.011",
      "\n",
      "[7,   607] loss: 0.011",
      "\n",
      "[8,   607] loss: 0.011",
      "\n",
      "[9,   607] loss: 0.011",
      "\n",
      "[10,   607] loss: 0.011",
      "\n",
      "[11,   607] loss: 0.011",
      "\n",
      "[12,   607] loss: 0.011",
      "\n",
      "[13,   607] loss: 0.011",
      "\n",
      "[14,   607] loss: 0.011",
      "\n",
      "[15,   607] loss: 0.011",
      "\n",
      "[16,   607] loss: 0.011",
      "\n",
      "[17,   607] loss: 0.011",
      "\n",
      "[18,   607] loss: 0.011",
      "\n",
      "[19,   607] loss: 0.011",
      "\n",
      "[20,   607] loss: 0.011",
      "\n",
      "[20,   607] loss: 0.011",
      "\n",
      "Finished Training",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model_no = Classifier(input_dim)\n",
    "\n",
    "criterion_no = nn.MSELoss()\n",
    "optimizer_no = optim.SGD(model_no.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(no_dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer_no.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_no(inputs)\n",
    "        loss = criterion_no(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_no.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()  # print every 2000 mini-batches\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "\n",
    "print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def intify(x):\n",
    "    x = x.detach().numpy()\n",
    "    for i, item in np.ndenumerate(x):\n",
    "        if item >= 1.5:\n",
    "            x[i] = 2\n",
    "        else:\n",
    "            x[i] = 1\n",
    "    return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[ 0.3189305  -0.4584872  -0.4016695  -0.24475643 -0.5882311 ]",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "num_features = len(features)\n",
    "datasize = np.power(2, num_features)\n",
    "dataset = np.zeros((datasize, num_features))\n",
    "syn_data_no = {}\n",
    "syn_data_yes = {}\n",
    "ATEs = torch.zeros(5)\n",
    "cnt = 0 \n",
    "for i, subset in enumerate(powerset(features)):\n",
    "    items = features.copy()\n",
    "    items = np.array([1 if item in subset else 2 for item in items]).reshape(1, -1)\n",
    "    dataset[i] = items\n",
    "    input_feat = torch.tensor(items).float()\n",
    "    outcome_no = model_no(input_feat)\n",
    "    syn_data_no[i] = outcome_no\n",
    "    outcome_yes = model_yes(input_feat)\n",
    "    syn_data_yes[i] = outcome_yes\n",
    "    ITE = outcome_yes - outcome_no\n",
    "    ATEs += ITE.view(-1)\n",
    "    cnt += 1\n",
    "ATE = (ATEs.detach()).numpy()\n",
    "ATE = np.divide(ATE, cnt)\n",
    "print(ATE)    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[ 0.32534176 -0.46107384 -0.41096179 -0.24751939 -0.59251805]",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def calc_error(row, row_not):\n",
    "    err = (row != row_not).sum()\n",
    "    return err\n",
    "\n",
    "def search_for_closest(my_feature, all_features, K):\n",
    "\n",
    "    lst_errors = []\n",
    "    for index, row_not_ft in enumerate(all_features):\n",
    "        err = calc_error(my_feature, row_not_ft)\n",
    "        if len(lst_errors) < K:\n",
    "            lst_errors.append([err, index])\n",
    "            lst_errors.sort(key=lambda x: x[0])\n",
    "        else:\n",
    "            if err >= lst_errors[-1][0]:\n",
    "                continue\n",
    "            lst_errors.pop(len(lst_errors)-1)\n",
    "            lst_errors.append([err, index])\n",
    "            lst_errors.sort(key=lambda x: x[0])\n",
    "    return lst_errors\n",
    "K = 15 \n",
    "ATE  = np.zeros(5)\n",
    "cnt = 0 \n",
    "for i, subset in enumerate(dataset):\n",
    "    my_closest = search_for_closest(subset, dataset, K)\n",
    "    ITE_nei = np.zeros(5)\n",
    "    for err, index in my_closest:\n",
    "        ITE_nei += (syn_data_no[index].detach()).numpy().reshape(-1)\n",
    "    ITE = np.divide(ITE_nei, K)\n",
    "    ATE += syn_data_yes[i].detach().numpy().reshape(-1) - ITE\n",
    "    cnt +=1 \n",
    "ATE = np.divide(ATE, cnt)\n",
    "print(ATE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TODO:\n",
    "1) To feed both networks with all possibiliteis of feature input.\n",
    "2) Create the synthetic data: 2*2^(num of features) outputs (one for pregnant and one for regular)\n",
    "    shape of a synthetic datum: (9, 5, 1) 9 features; 5 outputs, 1 label\n",
    "    Best way for the syntethic data is 2 dict where key is features and value is outputs\n",
    "    pregnant dict : {feature : output}\n",
    "    not pregnant dict : {feature : output}\n",
    "3) Apply 1NN on the synthetic data:\n",
    "    for each possibility of feature input f, we have m1(f) and m2(f).\n",
    "    calculate the ITE: m1(f) - m2(f) (where m1 is the pregnant women model) \n",
    "    sum and avarage all the ITEs in order to get the ATE.\n",
    "3) Compute ATE as before:\n",
    "    for each possibility of feature input f, calculate the ITE by avaraging the KNN\n",
    "    ITE = m1(f) - 1/K * Sum (m2(fi)) where fi are the KNN of f in the synthetic data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md \n",
     "is_executing": false
    }
   },
   "execution_count": 182,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-182-41b977b0d2ba>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    TODO:\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ],
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-182-41b977b0d2ba>, line 1)",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "goal_diff = np.zeros(len(final_outcomes))\n",
    "cnt = 0\n",
    "patient_type_results_file = open(RESULTS_FILE.format(selected_feature, \"patient_type\"), 'w', newline='')\n",
    "intubed_results_file = open(RESULTS_FILE.format(selected_feature, \"intubed\"), 'w', newline='')\n",
    "pneumonia_results_file = open(RESULTS_FILE.format(selected_feature, \"pneumonia\"), 'w', newline='')\n",
    "icu_results_file = open(RESULTS_FILE.format(selected_feature, \"icu\"), 'w', newline='')\n",
    "died_results_file = open(RESULTS_FILE.format(selected_feature, \"died\"), 'w', newline='')\n",
    "patient_type_writer = csv.writer(patient_type_results_file)\n",
    "intubed_writer = csv.writer(intubed_results_file)\n",
    "pneumonia_writer = csv.writer(pneumonia_results_file)\n",
    "icu_writer = csv.writer(icu_results_file)\n",
    "died_writer = csv.writer(died_results_file)\n",
    "all_writers = [patient_type_writer,\n",
    "               intubed_writer,\n",
    "               pneumonia_writer,\n",
    "               icu_writer,\n",
    "               died_writer,\n",
    "               ]\n",
    "write_in_all_csvs(all_writers, features + [\"Smoker\", \"Non-smoker\", \"diff\"])\n",
    "\n",
    "for subset in powerset(features):\n",
    "    items = features.copy()\n",
    "    items = np.array([1 if item in subset else 2 for item in items]).reshape(1, -1)\n",
    "    yes_result = logic_reg_yes.predict(items)[0]\n",
    "    no_result = logic_reg_no.predict(items)[0]\n",
    "    items = [True if i==1 else False for i in list(items[0])]\n",
    "    patient_type_writer.writerow(items + [yes_result[0], no_result[0], yes_result[0]-no_result[0]])\n",
    "    intubed_writer.writerow(items + [yes_result[1], no_result[1], yes_result[1]-no_result[1]])\n",
    "    pneumonia_writer.writerow(items + [yes_result[2], no_result[2], yes_result[2]-no_result[2]])\n",
    "    icu_writer.writerow(items + [yes_result[3], no_result[3], yes_result[3]-no_result[3]])\n",
    "    died_writer.writerow(items + [yes_result[4], no_result[4], yes_result[4]-no_result[4]])\n",
    "\n",
    "    goal_diff = np.add(np.subtract(yes_result, no_result), goal_diff)\n",
    "    cnt += 1\n",
    "goal_diff = np.divide(goal_diff, cnt)\n",
    "print(f\"---FINAL: { goal_diff}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(final_outcomes)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}